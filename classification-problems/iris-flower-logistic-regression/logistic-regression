
===================
Logistic Regression
===================
Logistic regression is another technique borrowed by machine learning from the field of statistics.

Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). 
Like all regression analyses, the logistic regression is a predictive analysis.
Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.

Types of Questions Binary Logistic Regression Can Answer
--> How does the probability of getting lung cancer (yes vs. no) change for every additional pound a person is overweight and for every pack of cigarettes smoked per day?
--> Do body weight, calorie intake, fat intake, and age have an influence on the probability of having a heart attack (yes vs. no)?

The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values.

Sigmoid Function is given by: Z = 1/(1 + exp(-arg))
	Where
		e = base of the natural logarithm
It’s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.

Exponential Function or [e^(x)] is given by: e^(x) = sum-of-terms-from-zero-to-infinity(((x)^n)/n!)

Logistic regression models the probability of the default class
Linear regression is not suitable for classification problem.
Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.

Binary Logistic Regression
	The categorical response has only two 2 possible outcomes. Example: Spam or Not
Multinomial Logistic Regression
	Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)
Ordinal Logistic Regression
	Three or more categories with ordering. Example: Movie rating from 1 to 5.

Hypothesis Representation
=========================
	H(x) = z(theta' * x)
	z(arg) = 1/(1 + exp(-arg))
	or
	P(y=0|x;) + P(y=1|x;) = 1  or
	H(x) = P(y=0|x;) = 1 - P(y=1|x;)
	Where
		H(x) gives us the probability that our output is 1.

In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:
	H(x) >= 0.5 → y = 1
	H(x) <  0.5 → y = 0

The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:
	z(arg) >= 0.5 when arg >= 0

The decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function.
Again, the input to the sigmoid function z(arg) doesn't need to be linear, and could be a function that describes a circle.

Cost Function
=============
We cannot use the cost function from linear regression because the Logistic Function will cause the output to be wavy, causing many local optima.
In other words, it will not be a convex function.
Instead, our cost function for logistic regression looks like:
	J(0) = (1/m) * (sum(Cost(H(x), y)))
	Where
		Cost(H(x), y) = -log(H(x)), 	if y = 1
		Cost(H(x), y) = -log(1 - H(x)), if y = 0

	Cost(H(x), y) = 0  if H(x) = y 
	Cost(H(x), y) → ∞ if y = 0 and H(x) → 1
	Cost(H(x), y) → ∞ if y = 1 and H(x) → 0

If our correct answer 'y' is 0, then the cost function will be 0 if our hypothesis function also outputs 0.
If our hypothesis approaches 1, then the cost function will approach infinity.

If our correct answer 'y' is 1, then the cost function will be 0 if our hypothesis function outputs 1.
If our hypothesis approaches 0, then the cost function will approach infinity.

Note that writing the cost function in this way guarantees that J(θ) is convex for logistic regression.

We can compress our cost function's two conditional cases into one case:
	Cost(H(x), y) = -y(log(H(x)) - (1 - y)(log(1 - H(x))

A vectorized implementation is given by:
	H(X) = z(Xθ)
	J(θ) = (1/m)*(−Y'log(H) − (1 − y)'log(1−h))

"Conjugate gradient", "BFGS", and "L-BFGS" are more sophisticated, faster ways to optimize θ that can be used instead of gradient descent.




Introduction
============
Logistic regression is a classification algorithm used to assign observations to a discrete set of classes.
Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.

Comparison to linear regression
===============================
Given data on time spent studying and exam scores. Linear Regression and logistic regression can predict different things:
-->	Linear Regression could help us predict the student’s test score on a scale of 0 - 100.
	Linear regression predictions are continuous (numbers in a range).

-->	Logistic Regression could help predict whether the student passed or failed.
	Logistic regression predictions are discrete (only specific values or categories are allowed).
	We can also view probability scores underlying the model’s classifications.

Types of logistic regression
============================
Binary (Pass/Fail)
Multi (Cats, Dogs, Sheep)
Ordinal (Low, Medium, High)

Sigmoid activation
==================
In order to map predicted values to probabilities, we use the sigmoid function.
The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.
	S(z) = (1/(1 + e^(−z)))
	Where
		s(z) = output between 0 and 1 (probability estimate)
		z = input to the function (your algorithm’s prediction e.g. mx + b)
		e = base of natural log

Decision boundary
=================
Our current prediction function returns a probability score between 0 and 1.
In order to map this to a discrete class (true/false), we select a threshold value above which we will classify values into class 1 and below which we classify values into class 2.
	p ≥ 0.5, then class = 1
	p < 0.5, then class = 0

For example, if our threshold was 0.5 and our prediction function returned 0.7, we would classify this observation as positive.
If our prediction was 0.2 we would classify the observation as negative.
For logistic regression with multiple classes we could select the class with the highest predicted probability.

Making predictions
==================
Using our knowledge of sigmoid functions and decision boundaries, we can now write a prediction function.
A prediction function in logistic regression returns the probability of our observation being Positive, True, or Yes.

Let’s use the same multiple linear regression equation from our linear regression tutorial.
	z = W0 + W1*Studied + W2*Slept
This time however we will transform the output using the sigmoid function to return a probability value between 0 and 1.
If the model returns 0.4 it believes there is only a 40% chance of passing.
If our decision boundary was 0.5, we would categorize this observation as Fail.

	def sigmoid(z):
  		return 1.0 / (1 + np.exp(-z))

  	def predict(features, weights):
		''' Returns 1D array of probabilities that the class label == 1 '''
	  	z = np.dot(features, weights)
	  	return sigmoid(z)

Unfortunately we can’t (or at least shouldn’t) use the same cost function MSE (L2) as we did for linear regression.
Because our prediction function is non-linear (due to sigmoid transform).
Squaring this prediction as we do in MSE results in a non-convex function with many local minimums.
If our cost function has many local minimums, gradient descent may not find the optimal global minimum.

Instead of Mean Squared Error, we use a cost function called Cross-Entropy, also known as Log Loss.
Cross-entropy loss can be divided into two separate cost functions: one for y=1 and one for y=0.
	Cost(H(x), y) = 0  if H(x) = y 
	Cost(H(x), y) → ∞ if y = 0 and H(x) → 1
	Cost(H(x), y) → ∞ if y = 1 and H(x) → 0

The benefits of taking the logarithm reveal themselves when we look at the cost function graphs for y=1 and y=0.
These smooth monotonic functions (always increasing or always decreasing) make it easy to calculate the gradient and minimize cost.

The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions.
The corollary is increasing prediction accuracy (closer to 0 or 1) has diminishing returns on reducing cost due to the logistic nature of our cost function.

We can compress our cost function's two conditional cases into one case:
	Cost(H(x), y) = -y(log(H(x)) - (1 - y)(log(1 - H(x))

A vectorized implementation is given by:
	H(X) = z(Xθ)
	J(θ) = (1/m)*(−Y'log(H) − (1 − y)'log(1−h))

Implementation for Cost Function
================================
	def cost_function(features, labels, weights):
	    ''' Using Mean Absolute Error 
	    	Features:(100,3)
	    	Labels: (100,1)
	    	Weights:(3,1)
	    	Returns 1D matrix of predictions
	    	Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels) '''

	    observations = len(labels)

	    predictions = predict(features, weights)

	    #Take the error when label=1
	    class1_cost = -labels*np.log(predictions)

	    #Take the error when label=0
	    class2_cost = (1-labels)*np.log(1-predictions)

	    #Take the sum of both costs
	    cost = class1_cost - class2_cost

	    #Take the average cost
	    cost = cost.sum() / observations

	    return cost

Gradient Descent for Logistic Regression
========================================
To minimize our cost, we use Gradient Descent just like before in Linear Regression.
There are other more sophisticated optimization algorithms out there such as conjugate gradient like BFGS, but we don’t have to worry about these.
Machine learning libraries like Scikit-learn hide their implementations so we can focus on more interesting things.

One of the neat properties of the sigmoid function is its derivative is easy to calculate.
	For the given sigmoid function: S(z) = (1/(1 + e^(−z)))
	The derivative is given by: S'(z) = S(z)(1 - S(z))
	Which leads to an equally beautiful and convenient cost function derivative: C' = X(S(z) − Y)
	Where
		C' is the derivative of cost with respect to weights
		Y is the actual class label (0 or 1)
		S(z) is our model’s prediction
		X is our feature or feature vector.
Notice how this gradient is the same as the MSE (L2) gradient, the only difference is the hypothesis function.

Implementation for Gradient Descent
===================================
	def update_weights(features, labels, weights, lr):
	    ''' Vectorized Gradient Descent
	    	Features:(200, 3)
	    	Labels: (200, 1)
	    	Weights:(3, 1) '''
	    
	    N = len(features)
	    
	    #1 - Get Predictions
	    predictions = predict(features, weights)
	    
	    #2 Transpose features from (200, 3) to (3, 200)
	    # So we can multiply w the (200,1)  cost matrix.
	    # Returns a (3,1) matrix holding 3 partial derivatives --
	    # one for each feature -- representing the aggregate
	    # slope of the cost function across all observations
	    gradient = np.dot(features.T,  predictions - labels)

	    #3 Take the average cost derivative for each feature
	    gradient /= N

	    #4 - Multiply the gradient by our learning rate
	    gradient *= lr

	    #5 - Subtract from our weights to minimize cost
	    weights -= gradient

	    return weights

The final step is assign class labels (0 or 1) to our predicted probabilities.
	def decision_boundary(prob):
		return 1 if prob >= .5 else 0

Convert probabilities to classes
	def classify(predictions):
  		'''
  			input  - N element array of predictions between 0 and 1
  			output - N element array of 0s (False) and 1s (True)
  		'''
  		decision_boundary = np.vectorize(decision_boundary)
  		return decision_boundary(predictions).flatten()

Accuracy measures how correct our predictions were. In this case we simply compare predicted labels to true labels and divide by the total.
	def accuracy(predicted_labels, actual_labels):
		diff = predicted_labels - actual_labels
		return 1.0 - (float(np.count_nonzero(diff)) / len(diff))

Another helpful technique is to plot the decision boundary on top of our predictions to see how our labels compare to the actual labels.
This involves plotting our predicted probabilities and coloring them with their true labels.
	def plot_decision_boundary(trues, falses):
		fig = plt.figure()
		ax = fig.add_subplot(111)

		no_of_preds = len(trues) + len(falses)

		ax.scatter([i for i in range(len(trues))], trues, s=25, c='b', marker="o", label='Trues')
		ax.scatter([i for i in range(len(falses))], falses, s=25, c='r', marker="s", label='Falses')

		plt.legend(loc='upper right');
		ax.set_title("Decision Boundary")
		ax.set_xlabel('N/2')
		ax.set_ylabel('Predicted Probability')
		plt.axhline(.5, color='black')
		plt.show()

Multiclass Logistic Regression
==============================
Instead of y = 0,1 we will expand our definition so that y = 0,1...n.
Basically we re-run binary classification multiple times, once for each class.

Procedure
	Divide the problem into n+1 binary classification problems (+1 because the index starts at 0?).
	For each class
	Predict the probability the observations are in that single class.
	prediction = max(probability of the classes)

For each sub-problem, we select one class (YES) and lump all the others into a second class (NO).
Then we take the class with the highest predicted value.

Softmax Activation or Softargmax or Normalized Exponential Function
===================================================================
It is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers.
That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval [ 0 , 1 ] , and the components will add up to 1, so that they can be interpreted as probabilities.

The standard (unit) softmax function is defined by the formula:
	softmax(z) = e^(z)/sum-of-K-terms(e(z))   for i=1,.,.,.,K and z=z1,.,.,.,zK

In words
We apply the standard exponential function to each element z(i) of the input vector z and normalize these values by dividing by the sum of all these exponentials;
This normalization ensures that the sum of the components of the output vector σ(z) or softmax(z) is 1.


