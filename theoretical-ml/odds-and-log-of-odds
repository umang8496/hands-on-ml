
================================
Log Odds for Logistic Regression
================================
Log odds play a central role in logistic regression.
Every probability can be easily converted to log odds, by finding the odds ratio and taking the logarithm.
Despite the relatively simple conversion, log odds can be a little esoteric. 
Probability, odds ratios and log odds are all the same thing, just expressed in different ways.

Probability is a measure of the likelihood of an event to occur.
Many events cannot be predicted with total certainty.
We can predict only the chance of an event to occur, for example how likely they are to happen, using it.
Probability can range in from 0 to 1, where 0 means the event to be an impossible one and 1 indicates a certain event.
The probability of all the events in a sample space adds up to 1.

To better understand the connections between the log-odds of an outcome, the odds of an outcome, and the probability of an outcome,
it is helpful to work with a range of values on one scale and convert it to the others.
It’s also helpful to visualize the relationships with some plots.

For example, if the probability of an event is 0.2, that
	the odds of the event occurring are, odds = (0.2/0.8) = 0.25
	the log-odds of the event occurring are, log(odds) = log(0.2/0.8) = -1.3863
	the probability can be reconstructed as, P = odds/(1+odds) = (0.25/1.25) = 0.2

Log odds is the logarithm of the odds.

The odds ratio is the probability of success/probability of failure.
As an equation, that’s P(A)/P(not A), where P(A) is the probability of A, and P(not A) the probability of ‘not A’ (complement of A).

Taking the logarithm of the odds ratio gives us the log odds of A, which can be written as
	log(A) = log(P(A)/P(not A))

Since the probability of an event happening, P(not A) is equal to the probability of an event not happening, 1 – P(A), we can write the log odds as
	log [p/(1-p)]
	Where:
		p = the probability of an event happening
		1 – p = the probability of an event not happening

When a function’s variable represents a probability it’s called the logit function.
We sometimes choose to use log odds instead of more basic probability measures because they’re so easily updated with new data.


==============
Logit Function
==============
In statistics, the logit function or the log-odds is the logarithm of the odds (p/1-p) where p is the probability.
It is a type of function that creates a map of probability values from (0,1) to (-inf,+inf).
It is the inverse of sigmoid function.

In deep learning, the term logits layer is popularly used for the last neuron layer of neural networks used for classification tasks,
which produce raw prediction values as real numbers ranging from (-inf,+inf).

If p is a probability, then p/(1 − p) is the corresponding odds; the logit of the probability is the logarithm of the odds is given by:
	logit(p) = log(p/(1-p)) = log(p) - log(1-p)

The logit function is the negative of the derivative of the binary entropy function.
The inverse-logit function (logistic function) is also sometimes referred to as the expit function.
The log-odds function of probabilities is often used in state estimation algorithms because of its numerical advantages in the case of small probabilities.
Instead of multiplying very small floating point numbers, log-odds probabilities can just be summed up to calculate the (log-odds) joint probability.


==========
Odds-Ratio
==========
An odds ratio (OR) is a statistic that quantifies the strength of the association between two events A and B.
The odds ratio is defined as the ratio of the odds of A in the presence of B and the odds of A in the absence of B.
or equivalently (due to symmetry), the ratio of the odds of B in the presence of A and the odds of B in the absence of A.

Two events are independent if and only if the Odds-Ratio equals 1, the odds of one event are the same in either the presence or absence of the other event.
If the OR is greater than 1, then A and B are associated (correlated) in the sense that, compared to the absence of B, the presence of B raises the odds of A, and symmetrically the presence of A raises the odds of B.
Conversely, if the OR is less than 1, then A and B are negatively correlated, and the presence of one event reduces the odds of the other event.


